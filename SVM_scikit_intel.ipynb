{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxY6mT8lD8R7",
        "outputId": "1fc240b2-b16c-4494-9158-41f10ea99f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Fouille de données\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Fouille de données"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import site\n",
        "sys.path.append(os.path.join(os.path.dirname(site.getsitepackages()[0]), \"site-packages\"))\n",
        "!pip install scikit-learn\n",
        "!pip install imblearn\n",
        "!pip install dask\n",
        "! python -m pip install scikit-learn-intelex\n",
        "!pip install fastparquet"
      ],
      "metadata": {
        "id": "Ywwzj4qNGVaV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5de9c9-cee1-46ae-b2ed-24c0c536e27c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.8/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.8/dist-packages (2022.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from dask) (21.3)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask) (2022.11.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask) (1.3.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask) (0.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask) (6.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask) (2.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->dask) (3.0.9)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn-intelex\n",
            "  Downloading scikit_learn_intelex-2023.0.1-py38-none-manylinux1_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting daal4py==2023.0.1\n",
            "  Downloading daal4py-2023.0.1-py38-none-manylinux1_x86_64.whl (12.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.8/dist-packages (from scikit-learn-intelex) (1.0.2)\n",
            "Collecting daal==2023.0.1\n",
            "  Downloading daal-2023.0.1-py2.py3-none-manylinux1_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from daal4py==2023.0.1->scikit-learn-intelex) (1.21.6)\n",
            "Collecting tbb==2021.*\n",
            "  Downloading tbb-2021.8.0-py2.py3-none-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.1.0)\n",
            "Installing collected packages: tbb, daal, daal4py, scikit-learn-intelex\n",
            "Successfully installed daal-2023.0.1 daal4py-2023.0.1 scikit-learn-intelex-2023.0.1 tbb-2021.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastparquet\n",
            "  Downloading fastparquet-2022.12.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from fastparquet) (1.21.6)\n",
            "Collecting pandas>=1.5.0\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from fastparquet) (2022.11.0)\n",
            "Collecting cramjam>=2.3\n",
            "  Downloading cramjam-2.6.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fastparquet) (21.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->fastparquet) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.15.0)\n",
            "Installing collected packages: cramjam, pandas, fastparquet\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed cramjam-2.6.2 fastparquet-2022.12.0 pandas-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn(\"SVC\")\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "import dask.dataframe as dd\n",
        "import fastparquet\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "train = pd.read_parquet('train.parquet.gzip')\n",
        "test = pd.read_parquet('test.parquet.gzip')\n",
        "\n",
        "Xtrain = train.drop([\"FlagImpaye\", \"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", \"CodeDecision\", \"Unnamed: 0\"], axis = 1)\n",
        "\n",
        "Ytrain = pd.DataFrame(train.FlagImpaye)\n",
        "Ytrain = train['FlagImpaye'].astype('int')\n",
        "\n",
        "Xtest  = test.drop([\"FlagImpaye\",\"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\",\"CodeDecision\", 'Unnamed: 0'], axis = 1)\n",
        "Ytest  = test.FlagImpaye"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsffEi_GeaOI",
        "outputId": "c49fde1a-e3d9-436a-d8e4-038516eed590"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import average_precision_score, confusion_matrix\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Application des algorithmes \n",
        "\n",
        "def applyAlgo(algo, Xtrain, Ytrain, Xtest, Ytest):\n",
        "\n",
        "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
        "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
        "\n",
        "    algo = SVC()\n",
        "    algo.fit(Xtrain, Ytrain)\n",
        "    rankTrain = algo.predict(Xtrain)\n",
        "    rankTest = algo.predict(Xtest)\n",
        "\n",
        "   # Cette deuxième partie permet d'indiquer quelle est la mesure de performance que \n",
        "   # vous souhaitez considérer pour votre étude en cours\n",
        "    ctest = confusion_matrix(Ytest, rankTest)\n",
        "    ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
        "\n",
        "    return (ftest)\n",
        "\n",
        "def Echantillon(number, Xtrain, Ytrain):\n",
        "    Xtrain = Xtrain.sample(n = number)\n",
        "    Ytrain = Ytrain.sample(n = number)\n",
        "\n",
        "    return number, Xtrain, Ytrain\n",
        "\n",
        "def Processing(method, Xtrain, Ytrain, Xtest):\n",
        "\n",
        "    if method == \"NoProcessing\":\n",
        "        pass\n",
        "    \n",
        "    if method == \"SelectFromModel\": \n",
        "        clf = ExtraTreesClassifier(n_estimators=50)\n",
        "        clf = clf.fit(Xtrain, Ytrain)\n",
        "\n",
        "        model = SelectFromModel(clf, prefit=True)\n",
        "\n",
        "        Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
        "        Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
        "\n",
        "    if method == \"Variance\": \n",
        "        selector = VarianceThreshold(threshold = 0.8)\n",
        "        selector.fit_transform(Xtrain)\n",
        "\n",
        "        Xtrain = Xtrain[Xtrain.columns[selector.get_support(indices = True)]]\n",
        "        Xtest = Xtest[Xtest.columns[selector.get_support(indices = True)]]\n",
        "\n",
        "    return method, Xtrain, Xtest\n",
        "\n",
        "def SMOTE_S(sampling, Xtrain, Ytrain):\n",
        "\n",
        "    sm = BorderlineSMOTE(sampling_strategy = sampling,random_state = 0)\n",
        "\n",
        "    Xtrain, Ytrain = sm.fit_resample(Xtrain, Ytrain)\n",
        "    \n",
        "    return sampling, Xtrain, Ytrain\n"
      ],
      "metadata": {
        "id": "YHtmmKzmfB9Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['SVC']\n",
        "number = [100000]  #380000 , 999999]\n",
        "method = ['NoProcessing', 'Variance']\n",
        "sampling = [0, 0.3 , 0.5]\n",
        "\n",
        "import time\n",
        "Fmesure = []\n",
        "execution = []\n",
        "algo = []\n",
        "n_data = []\n",
        "variable_selector = []\n",
        "smote = []\n",
        "\n",
        "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Sampling\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n",
        "\n",
        "for s in sampling :\n",
        "    for x in number :\n",
        "        for l in method : \n",
        "            for model in models : \n",
        "\n",
        "                if s == 0:\n",
        "                  x, X_sample_train, Y_sample_train = Echantillon(x, Xtrain, Ytrain)\n",
        "                  n_data.append(x)\n",
        "                else:\n",
        "                  s, X_sample_train, Y_sample_train = SMOTE_S(s, Xtrain, Ytrain)\n",
        "                  smote.append(s)\n",
        "\n",
        "                  x, X_sample_train, Y_sample_train = Echantillon(x, X_sample_train, Y_sample_train)\n",
        "                  n_data.append(x)\n",
        "\n",
        "\n",
        "                l, X_sample_train, X_sample_test = Processing(l, X_sample_train, Y_sample_train, Xtest)\n",
        "                variable_selector.append(l)\n",
        "\n",
        "                # Stocker l'algorithme qui tourne\n",
        "                algo.append(model)\n",
        "                start = time.time()\n",
        "                # Fin normalisation\n",
        "                apTest = applyAlgo(model, X_sample_train, Y_sample_train, X_sample_test, Ytest)\n",
        "                Fmesure.append(apTest) \n",
        "                end = time.time()\n",
        "                elapsed = end - start \n",
        "                execution.append(elapsed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0IKmZBCrf-_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n"
      ],
      "metadata": {
        "id": "XIvxTzdCoc_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_df = pd.DataFrame(list(zip(algo, n_data, variable_selector, Fmesure, smote, execution)), columns= columns) \n",
        "score_df.to_csv(\"resultats_SVM.csv\")"
      ],
      "metadata": {
        "id": "0anf4kI6oQy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}