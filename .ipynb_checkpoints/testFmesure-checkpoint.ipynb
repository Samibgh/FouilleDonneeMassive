{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceaddc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import dask.dataframe as dd\n",
    "import fastparquet\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "try:\n",
    "    os.chdir(\"C:/Users/sibghi/Downloads/wetransfer_test-parquet-gzip_2023-01-12_1210\")\n",
    "except:\n",
    "    os.chdir(\"/Users/titouanhoude/Documents/GitHub\")\n",
    "    \n",
    "train = pd.read_parquet('train.parquet.gzip')\n",
    "test = pd.read_parquet('test.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6abbcf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ddd903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "classes = np.unique(train.FlagImpaye)\n",
    "cw = class_weight.compute_class_weight(class_weight = 'balanced',classes =  np.unique(train.FlagImpaye),y= train.FlagImpaye)\n",
    "weights = dict(zip(classes,cw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c6233d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = train.sample(n = 100000)\n",
    "#test_test = test.sample(n = 100000)\n",
    "\n",
    "Xtrain = train_test.drop([\"FlagImpaye\", \"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", \"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "Ytrain = pd.DataFrame(train_test.FlagImpaye)\n",
    "Ytrain = train_test['FlagImpaye'].astype('int')\n",
    "\n",
    "Xtest  = test.drop([\"FlagImpaye\",\"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", 'Unnamed: 0'], axis = 1)\n",
    "Ytest  = test.FlagImpaye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5fe8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={'xgboost': XGBRFClassifier(random_state=0),\n",
    "       'RandomForest': RandomForestClassifier(random_state=0),\n",
    "       'DecisionTree': DecisionTreeClassifier(random_state=0),\n",
    "       'GradientBoosting' : GradientBoostingClassifier(random_state=0), \n",
    "       }\n",
    "names=[]\n",
    "f1score_ =[]\n",
    "f_mesure = []\n",
    "\n",
    "\n",
    "sm = BorderlineSMOTE(sampling_strategy = 0.3,random_state = 0)\n",
    "\n",
    "XBdSmote , YBdSmote = sm.fit_resample(Xtrain, Ytrain)\n",
    "\n",
    "for name, model in models.items(): \n",
    "          \n",
    "    name_model = model\n",
    "    name_fit = name_model.fit(XBdSmote , YBdSmote)\n",
    "    name_pred = name_fit.predict(Xtest)\n",
    "    ctest = confusion_matrix(Ytest, name_pred)\n",
    "    ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    f1score = f1_score(Ytest,name_pred, average = \"macro\")\n",
    "    names.append(name)\n",
    "    f1score_.append(f1score)\n",
    "    f_mesure.append(ftest)\n",
    "    \n",
    "score_df = pd.DataFrame(list(zip(names, f1score_, f_mesure)))\n",
    "score_df.columns = [\"Nom\", \"Score\",\"f_mesure\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0659fbdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Score</th>\n",
       "      <th>f_mesure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.889023</td>\n",
       "      <td>0.7797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.889266</td>\n",
       "      <td>0.7802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.496964</td>\n",
       "      <td>0.0755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.891153</td>\n",
       "      <td>0.7840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Nom     Score  f_mesure\n",
       "0           xgboost  0.889023    0.7797\n",
       "1      RandomForest  0.889266    0.7802\n",
       "2      DecisionTree  0.496964    0.0755\n",
       "3  GradientBoosting  0.891153    0.7840"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ff0e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test with weights\n",
    "models={'xgboost': XGBRFClassifier(random_state=0, scale_pos_weight =100),\n",
    "       'RandomForest': RandomForestClassifier(random_state=0,class_weight=weights),\n",
    "       'DecisionTree': DecisionTreeClassifier(random_state=0,class_weight=weights),\n",
    "       'GradientBoosting' : GradientBoostingClassifier(random_state=0), \n",
    "       }\n",
    "names=[]\n",
    "f1score_ =[]\n",
    "f_mesure = []\n",
    "\n",
    "for name, model in models.items(): \n",
    "          \n",
    "    name_model = model\n",
    "    if name ==\"GradientBoosting\":\n",
    "        sample_weight = np.array([10 if i == -1 else 1 for i in Ytrain])\n",
    "        name_fit = name_model.fit(Xtrain, Ytrain,sample_weight)\n",
    "    else:\n",
    "        name_fit = name_model.fit(Xtrain, Ytrain)\n",
    "        \n",
    "    name_pred = name_fit.predict(Xtest)\n",
    "    m = confusion_matrix(Ytest, name_pred)\n",
    "    f = (2 * m[0][0]) / ((2 * m[0][0]) + m[0][1] + m[1][0])\n",
    "\n",
    "    \n",
    "    f1score = f1_score(Ytest,name_pred, average = \"macro\")\n",
    "    names.append(name)\n",
    "    f1score_.append(f1score)\n",
    "    f_mesure.append(f)\n",
    "    \n",
    "score_df2 = pd.DataFrame(list(zip(names, f1score_, f_mesure)))\n",
    "score_df2.columns = [\"Nom\", \"Score\",\"f_mesure\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "549aeeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Score</th>\n",
       "      <th>f_mesure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.661069</td>\n",
       "      <td>0.986683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.888188</td>\n",
       "      <td>0.998439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.851026</td>\n",
       "      <td>0.997682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.819962</td>\n",
       "      <td>0.996786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Nom     Score  f_mesure\n",
       "0           xgboost  0.661069  0.986683\n",
       "1      RandomForest  0.888188  0.998439\n",
       "2      DecisionTree  0.851026  0.997682\n",
       "3  GradientBoosting  0.819962  0.996786"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78fefc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from xgboost import XGBRFClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "l = [0.3,0.4,0.5,0.6, 0.7]\n",
    "for i in l :\n",
    "\n",
    "    sm = BorderlineSMOTE(sampling_strategy = i, random_state=0)\n",
    "    X_sm, y_sm = sm.fit_resample(Xtrain, Ytrain)\n",
    "    \n",
    "    model = XGBRFClassifier().fit(X_sm, y_sm)\n",
    "    \n",
    "    pickle.dump(model, open(\"C:/Users/sibghi/Documents/GitHub/FouilleDonneeMassive/Pickel_model/model_\"+ str(i), 'wb'))\n",
    "\n",
    "    \n",
    "    y_pred = model.predict(Xtest)\n",
    "    m = confusion_matrix(Ytest, y_pred)\n",
    "    \n",
    "    pickle.dump(m, open(\"C:/Users/sibghi/Documents/GitHub/FouilleDonneeMassive/Pickel_model/matrix_\"+ str(i), 'wb'))\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6239dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[99061,    97],\n",
       "       [  286,   556]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(\"C:/Users/sibghi/Documents/GitHub/FouilleDonneeMassive/Pickel_model/model_0.3\", 'rb'))\n",
    "\n",
    "y_pred = loaded_model.predict(Xtest)\n",
    "m = confusion_matrix(Ytest, y_pred)\n",
    "m[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca36363f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98832,   326],\n",
       "       [  279,   563]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f524c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99158\n",
      "           1       0.85      0.66      0.74       842\n",
      "\n",
      "    accuracy                           1.00    100000\n",
      "   macro avg       0.92      0.83      0.87    100000\n",
      "weighted avg       1.00      1.00      1.00    100000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99158\n",
      "           1       0.78      0.67      0.72       842\n",
      "\n",
      "    accuracy                           1.00    100000\n",
      "   macro avg       0.89      0.83      0.86    100000\n",
      "weighted avg       1.00      1.00      1.00    100000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99158\n",
      "           1       0.57      0.67      0.62       842\n",
      "\n",
      "    accuracy                           0.99    100000\n",
      "   macro avg       0.79      0.83      0.81    100000\n",
      "weighted avg       0.99      0.99      0.99    100000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99158\n",
      "           1       0.60      0.67      0.63       842\n",
      "\n",
      "    accuracy                           0.99    100000\n",
      "   macro avg       0.80      0.83      0.81    100000\n",
      "weighted avg       0.99      0.99      0.99    100000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     99158\n",
      "           1       0.63      0.67      0.65       842\n",
      "\n",
      "    accuracy                           0.99    100000\n",
      "   macro avg       0.82      0.83      0.82    100000\n",
      "weighted avg       0.99      0.99      0.99    100000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for i in l : \n",
    "    loaded_model = pickle.load(open(\"C:/Users/sibghi/Documents/GitHub/FouilleDonneeMassive/Pickel_model/model_\" +str(i), 'rb'))\n",
    "    y_pred = loaded_model.predict(Xtest)\n",
    "    tab = classification_report(Ytest, y_pred)\n",
    "    print(tab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35949a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models={'xgboost': XGBRFClassifier(learning_rate : ),listP({\"k\": [1]})\n",
    "       'RandomForest': RandomForestClassifier(random_state=0),\n",
    "       'DecisionTree': DecisionTreeClassifier(random_state=0),\n",
    "       'GradientBoosting' : GradientBoostingClassifier(random_state=0), \n",
    "       }\n",
    "\n",
    "def listP(dic):  \n",
    "    params = list(dic.keys())\n",
    "    listParam = [{params[0]: value} for value in dic[params[0]]]\n",
    "    for i in range(1, len(params)):\n",
    "        newListParam = []\n",
    "        currentParamName = params[i]\n",
    "        currentParamRange = dic[currentParamName]\n",
    "        for previousParam in listParam:\n",
    "            for value in currentParamRange:\n",
    "                newParam = previousParam.copy()\n",
    "                newParam[currentParamName] = value\n",
    "                newListParam.append(newParam)\n",
    "        listParam = newListParam.copy()\n",
    "    return listParam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c673316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyAlgo(algo, p, Xtrain, ytrain, Xtest, ytest):\n",
    "\n",
    "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
    "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
    "\n",
    "    if algo == \"xgboost\":\n",
    "        clf = XGBRFClassifier()\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"RandomForest\":\n",
    "        clf = svm.SVC(C = p[\"C\"], kernel = 'linear')\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"svm_poly\":\n",
    "        clf = svm.SVC(C = p[\"C\"], degree = p[\"d\"] , kernel = 'poly')\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "   # Cette deuxième partie permet d'indiquer qu'elle est la mesure de performance que \n",
    "   # vous souhaitez considérer pour votre étude en cours\n",
    "\n",
    "\n",
    "    if sys.argv[2] == \"f1\": # La f-mesure\n",
    "\n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(2*ctrain[1,1]/(2*ctrain[1,1]+ctrain[0,1]+ctrain[1,0]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    elif sys.argv[2] == \"g1\": # La G-mesure\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(np.sqrt( (ctrain[1,1]/(ctrain[1,1]+ctrain[0,1]))*(ctrain[1,1]/(ctrain[1,1]+ctrain[1,0])) ),4)\n",
    "        if np.isnan(ftrain):\n",
    "            ftrain = 0\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(np.sqrt( (ctest[1,1]/(ctest[1,1]+ctest[0,1]))*(ctest[1,1]/(ctest[1,1]+ctest[1,0])) ),4)\n",
    "        if np.isnan(ftest):\n",
    "            ftest = 0\n",
    "\n",
    "    elif sys.argv[2] == \"gm\": # La G-mean\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round(np.sqrt( (ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]))*(ctrain[0,0]/(ctrain[0,0]+ctrain[0,1]))),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round(np.sqrt( (ctest[1,1]/(ctest[1,1]+ctest[1,0]))*(ctest[0,0]/(ctest[0,0]+ctest[0,1]))),4)\n",
    "\n",
    "    elif sys.argv[2] == \"ba\": # La Balanced Accuracy\n",
    "        \n",
    "        ctrain = confusion_matrix(ytrain, rankTrain)\n",
    "        ftrain = round( ctrain[1,1]/(ctrain[1,1]+ctrain[1,0]) + ctrain[0,0]/(ctrain[0,0]+ctrain[0,1]),4)\n",
    "        ctest = confusion_matrix(ytest, rankTest)\n",
    "        ftest = round((ctest[1,1]/(ctest[1,1]+ctest[1,0])+ctest[0,0]/(ctest[0,0]+ctest[0,1]))/2,4)\n",
    "\n",
    "\n",
    "    return (ftrain*100, ftest*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['glass', 'wine']:\n",
    "\n",
    "    X, y = data_recovery(dataset) # chargement des données\n",
    "    pctPos = 100*len(y[y == 1])/len(y) # calcul % positifs\n",
    "    dataset = \"{:05.2f}%\".format(pctPos) + \" \" + dataset \n",
    "    print(dataset, X.shape)\n",
    "    print(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, shuffle=True,\n",
    "                                                    stratify=y, test_size=0.2) # train-test split\n",
    "    skf = StratifiedKFold(n_splits=nbFoldValid, shuffle=True) # initialisation CV\n",
    "    foldsTrainValid = list(skf.split(Xtrain, ytrain)) # création des groupes\n",
    "    results[dataset] = {} \n",
    "    for algo in listParams.keys(): # on parcourt l'ensemble des algorithmes\n",
    "        if len(listParams[algo]) > 1:  # Début Cross-Validation\n",
    "            validParam = [] # Stockage du score moyen par éléments de notre grille hyper-paramètres\n",
    "            for param in listParams[algo]: # On parcourt l'ensemble des hyper-paramètres\n",
    "                valid = [] # stockage des scores pour sur chaque fold\n",
    "                for iFoldVal in range(nbFoldValid):\n",
    "                    fTrain, fValid = foldsTrainValid[iFoldVal] # définition du fold de validation\n",
    "                    # Normalisation\n",
    "                    normalizer = Normalizer() \n",
    "                    normalizer.fit(Xtrain[fTrain]) \n",
    "                    X_trainv = normalizer.transform(Xtrain[fTrain]) \n",
    "                    X_valid = normalizer.transform(Xtrain[fValid])\n",
    "                    # Fin normalisation\n",
    "                    # On applique notre algo \n",
    "                    valid.append(applyAlgo(algo, param,\n",
    "                                           X_trainv, ytrain[fTrain],\n",
    "                                           X_valid, ytrain[fValid])[1])\n",
    "                    # On stocke les valeurs \n",
    "                validParam.append(np.mean(valid))\n",
    "            param = listParams[algo][np.argmax(validParam)]\n",
    "        else:  # Pas de Cross-validation\n",
    "            param = listParams[algo][0]\n",
    "        # Normalisation\n",
    "        normalizer = Normalizer()\n",
    "        normalizer.fit(Xtrain)\n",
    "        Xtrain = normalizer.transform(Xtrain)\n",
    "        Xtest = normalizer.transform(Xtest)\n",
    "        # Fin normalisation\n",
    "        apTrain, apTest = applyAlgo(algo, param, Xtrain, ytrain, Xtest, ytest)\n",
    "        results[dataset][algo] = (apTrain, apTest)\n",
    "\n",
    "\n",
    "\n",
    "        # La suite n'est pas \"importante\" elle permettra de stocker vos résultats \n",
    "        # dans un tableau vous permettant de générer un pdf avec vos résultats\n",
    "\n",
    "    #if not os.path.exists(\"results\"):\n",
    "    #    try:\n",
    "    #        os.makedirs(\"results\")\n",
    "    #    except:\n",
    "    #        pass\n",
    "    #f = gzip.open(\"./results/res\" + str(seed) + \".pklz\", \"wb\")\n",
    "    #pickle.dump(results, f)\n",
    "    #f.close()\n",
    "\n",
    "# Affichage de l'ensemble des résultats\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
