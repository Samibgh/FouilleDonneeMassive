{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a638260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import dask.dataframe as dd\n",
    "import fastparquet\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "try:\n",
    "    os.chdir(\"C:/Users/Sam/Documents/SISE/Fouille de données\")\n",
    "except:\n",
    "    os.chdir(\"/Users/titouanhoude/Documents/GitHub\")\n",
    "    \n",
    "train = pd.read_parquet('train.parquet.gzip')\n",
    "test = pd.read_parquet('test.parquet.gzip')\n",
    "\n",
    "Xtrain = train.drop([\"FlagImpaye\", \"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", \"CodeDecision\", \"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "Ytrain = pd.DataFrame(train.FlagImpaye)\n",
    "Ytrain = train['FlagImpaye'].astype('int')\n",
    "\n",
    "Xtest  = test.drop([\"FlagImpaye\",\"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\",\"CodeDecision\", 'Unnamed: 0'], axis = 1)\n",
    "Ytest  = test.FlagImpaye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5c2083f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# clf = ExtraTreesClassifier(n_estimators=50)\n",
    "# clf = clf.fit(Xtrain, Ytrain)\n",
    "# clf.feature_importances_\n",
    "\n",
    "# model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "# Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "# Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "# Xtrain = model.transform(train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "59539fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "\n",
    "# nbFoldValid = 5 # nombre de groupes pour la K-CV\n",
    "\n",
    "# models2={#'SVC': SVC(),\n",
    "#        'RandomForest': RandomForestClassifier(random_state=0),\n",
    "#        'XGBRFClassifier' : XGBRFClassifier(random_state=0),\n",
    "#        'GradientBoosting' : GradientBoostingClassifier(random_state=0),\n",
    "#         'GradientBoosting' : GradientBoostingClassifier(random_state=0)\n",
    "#        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1e00dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application des algorithmes \n",
    "\n",
    "def applyAlgo(algo, Xtrain, Ytrain, Xtest, Ytest):\n",
    "\n",
    "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
    "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
    "\n",
    "    if algo == \"SVC\":\n",
    "        clf = SVC()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"GradientBoostingClassifier\":\n",
    "        clf = GradientBoostingClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"AdaBoostClassifier\":\n",
    "        clf = AdaBoostClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"XGBRFClassifier\":\n",
    "        clf = XGBRFClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "   # Cette deuxième partie permet d'indiquer quelle est la mesure de performance que \n",
    "   # vous souhaitez considérer pour votre étude en cours\n",
    "    ctest = confusion_matrix(Ytest, rankTest)\n",
    "    ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    return (ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c487a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Echantillon(number, Xtrain, Ytrain):\n",
    "    Xtrain = Xtrain.sample(n = number)\n",
    "    Ytrain = Ytrain.sample(n = number)\n",
    "\n",
    "    return number, Xtrain, Ytrain\n",
    "\n",
    "def Processing(method, Xtrain, Ytrain, Xtest):\n",
    "\n",
    "    if method == \"NoProcessing\":\n",
    "        pass\n",
    "    \n",
    "    if method == \"SelectFromModel\": \n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "        model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "    if method == \"Variance\": \n",
    "        selector = VarianceThreshold(threshold = 0.8)\n",
    "        selector.fit_transform(Xtrain)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[selector.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[selector.get_support(indices = True)]]\n",
    "\n",
    "    return method, Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1ececb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "# from functions import loadCsv, oneHotEncodeColumns, data_recovery\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Normalisation\n",
    "# normalizer = Normalizer()\n",
    "# normalizer.fit(Xtrain)\n",
    "# Xtrain = normalizer.transform(Xtrain)\n",
    "# Xtest = normalizer.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "64fab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest', 'XGBRFClassifier' , 'GradientBoostingClassifier', 'AdaBoostClassifier']# 'SVC']\n",
    "number = [100000, 380000 , 999999]\n",
    "method = ['NoProcessing', 'SelectFromModel' , 'Variance']\n",
    "import time\n",
    "Fmesure = []\n",
    "execution = []\n",
    "algo = []\n",
    "n_data = []\n",
    "variable_selector = []\n",
    "\n",
    "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n",
    "\n",
    "for x in number :\n",
    "    for l in method : \n",
    "        for model in models : \n",
    "\n",
    "            x, X_sample_train, Y_sample_train = Echantillon(x, Xtrain, Ytrain)\n",
    "            n_data.append(x)\n",
    "            \n",
    "            l, X_sample_train, X_sample_test = Processing(l, X_sample_train, Y_sample_train, Xtest)\n",
    "            variable_selector.append(l)\n",
    "\n",
    "            # Stocker l'algorithme qui tourne\n",
    "            algo.append(model)\n",
    "            start = time.time()\n",
    "            # Fin normalisation\n",
    "            apTest = applyAlgo(model, X_sample_train, Y_sample_train, X_sample_test, Ytest)\n",
    "            Fmesure.append(apTest) \n",
    "            end = time.time()\n",
    "            elapsed = end - start \n",
    "            execution.append(elapsed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c54f634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "9939fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame(list(zip(algo, n_data, variable_selector, Fmesure, execution)), columns= columns) \n",
    "score_df.to_csv(\"resultats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for0.3:0.0\n",
      "for0.4:0.0\n"
     ]
    }
   ],
   "source": [
    "#test the sampling strategy\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "x, X_sample_train, Y_sample_train = Echantillon(999999, Xtrain, Ytrain)\n",
    "\n",
    "l = [0.3,0.4,0.5,0.6]\n",
    "for i in l : \n",
    "    sm = BorderlineSMOTE(sampling_strategy = i, random_state=0)\n",
    "    X_res, y_res = sm.fit_resample(X_sample_train, Y_sample_train)\n",
    "    \n",
    "    cl = GradientBoostingClassifier().fit(X_res, y_res)\n",
    "    \n",
    "    pred = cl.predict(Xtest)\n",
    "    \n",
    "    ctest = confusion_matrix(Ytest, pred)\n",
    "    ftest = 2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0])\n",
    "    print(\"for\" + str(i) +\":\" + str(ftest))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5448824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(random_state=0)\n",
    "parameters = {'learning_rate': [0.01,0.02,0.03],\n",
    "                  'subsample'    : [0.7, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [4,6,8]}\n",
    "\n",
    "\n",
    "sm = BorderlineSMOTE(sampling_strategy = 0.5, random_state=0)\n",
    "X_res, y_res = sm.fit_resample(X_sample_train, Y_sample_train)\n",
    "\n",
    "cv = KFold(n_splits=7, shuffle=False)\n",
    "\n",
    "grid_gb = GridSearchCV(\n",
    "    estimator = clf,\n",
    "    param_grid = parameters,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = 4,\n",
    "    cv = cv,\n",
    "    refit = True,\n",
    "    return_train_score = True).fit(X_sample_train, Y_sample_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588348a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    " \n",
    "bag = BaggingClassifier(estimator = GradientBoostingClassifier(**grid_gb.best_params_ ),n_estimators=10, random_state=0)\n",
    "fit(X_res, y_res)\n",
    "\n",
    "pickle.dump(bag, open('modelGradient.pkl', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "333d2c9757a75282c9d4867cccb346dd97fbdd6438562bec117fa31e687fe23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
