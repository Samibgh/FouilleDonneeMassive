{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a638260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import dask.dataframe as dd\n",
    "import fastparquet\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "try:\n",
    "    os.chdir(\"C:/Users/Sam/Documents/SISE/Fouille de données\")\n",
    "except:\n",
    "    os.chdir(\"/Users/titouanhoude/Documents/GitHub\")\n",
    "    \n",
    "train = pd.read_parquet('train.parquet.gzip')\n",
    "test = pd.read_parquet('test.parquet.gzip')\n",
    "\n",
    "Xtrain = train.drop([\"FlagImpaye\", \"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", \"CodeDecision\", \"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "Ytrain = pd.DataFrame(train.FlagImpaye)\n",
    "Ytrain = train['FlagImpaye'].astype('int')\n",
    "\n",
    "Xtest  = test.drop([\"FlagImpaye\",\"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\",\"CodeDecision\", 'Unnamed: 0'], axis = 1)\n",
    "Ytest  = test.FlagImpaye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c2083f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# clf = ExtraTreesClassifier(n_estimators=50)\n",
    "# clf = clf.fit(Xtrain, Ytrain)\n",
    "# clf.feature_importances_\n",
    "\n",
    "# model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "# Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "# Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "# Xtrain = model.transform(train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59539fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "\n",
    "# nbFoldValid = 5 # nombre de groupes pour la K-CV\n",
    "\n",
    "# models2={#'SVC': SVC(),\n",
    "#        'RandomForest': RandomForestClassifier(random_state=0),\n",
    "#        'XGBRFClassifier' : XGBRFClassifier(random_state=0),\n",
    "#        'GradientBoosting' : GradientBoostingClassifier(random_state=0),\n",
    "#         'GradientBoosting' : GradientBoostingClassifier(random_state=0)\n",
    "#        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e00dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application des algorithmes \n",
    "\n",
    "def applyAlgo(algo, Xtrain, Ytrain, Xtest, Ytest):\n",
    "\n",
    "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
    "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
    "\n",
    "    if algo == \"SVC\":\n",
    "        clf = SVC()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"GradientBoostingClassifier\":\n",
    "        clf = GradientBoostingClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"AdaBoostClassifier\":\n",
    "        clf = AdaBoostClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"XGBRFClassifier\":\n",
    "        clf = XGBRFClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "   # Cette deuxième partie permet d'indiquer quelle est la mesure de performance que \n",
    "   # vous souhaitez considérer pour votre étude en cours\n",
    "    ctest = confusion_matrix(Ytest, rankTest)\n",
    "    ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    return (ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c487a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Echantillon(number, Xtrain, Ytrain):\n",
    "    Xtrain = Xtrain.sample(n = number)\n",
    "    Ytrain = Ytrain.sample(n = number)\n",
    "\n",
    "    return number, Xtrain, Ytrain\n",
    "\n",
    "def Processing(method, Xtrain, Ytrain, Xtest):\n",
    "\n",
    "    if method == \"NoProcessing\":\n",
    "        pass\n",
    "    \n",
    "    if method == \"SelectFromModel\": \n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "        model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "    if method == \"Variance\": \n",
    "        selector = VarianceThreshold(threshold = 0.8)\n",
    "        selector.fit_transform(Xtrain)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[selector.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[selector.get_support(indices = True)]]\n",
    "\n",
    "    return method, Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fec303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_S(sampling, Xtrain, Ytrain):\n",
    "\n",
    "    sm = BorderlineSMOTE(sampling_strategy = sampling,random_state = 0)\n",
    "\n",
    "    Xtrain, Ytrain = sm.fit_resample(Xtrain, Ytrain)\n",
    "    \n",
    "    return sampling, Xtrain, Ytrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ececb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "# from functions import loadCsv, oneHotEncodeColumns, data_recovery\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Normalisation\n",
    "# normalizer = Normalizer()\n",
    "# normalizer.fit(Xtrain)\n",
    "# Xtrain = normalizer.transform(Xtrain)\n",
    "# Xtest = normalizer.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64fab815",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['RandomForest', 'XGBRFClassifier' , 'GradientBoostingClassifier', 'AdaBoostClassifier']# 'SVC']\n",
    "number = [100000, 380000 , 999999, 3899362]\n",
    "method = ['NoProcessing', 'SelectFromModel' , 'Variance']\n",
    "sampling = [0.15, 0.3 , 0.5]\n",
    "\n",
    "import time\n",
    "Fmesure = []\n",
    "execution = []\n",
    "algo = []\n",
    "n_data = []\n",
    "variable_selector = []\n",
    "smote = []\n",
    "\n",
    "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Sampling\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n",
    "\n",
    "for s in sampling :\n",
    "    for x in number :\n",
    "        for l in method : \n",
    "            for model in models : \n",
    "\n",
    "                s, Xtrain, Ytrain = SMOTE_S(s, Xtrain, Ytrain)\n",
    "                smote.append(s)\n",
    "\n",
    "                x, X_sample_train, Y_sample_train = Echantillon(x, Xtrain, Ytrain)\n",
    "                n_data.append(x)\n",
    "                \n",
    "                l, X_sample_train, X_sample_test = Processing(l, X_sample_train, Y_sample_train, Xtest)\n",
    "                variable_selector.append(l)\n",
    "\n",
    "                # Stocker l'algorithme qui tourne\n",
    "                algo.append(model)\n",
    "                start = time.time()\n",
    "                # Fin normalisation\n",
    "                apTest = applyAlgo(model, X_sample_train, Y_sample_train, X_sample_test, Ytest)\n",
    "                Fmesure.append(apTest) \n",
    "                end = time.time()\n",
    "                elapsed = end - start \n",
    "                execution.append(elapsed)\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(list(zip(algo, n_data, variable_selector, Fmesure, smote, execution)), columns= columns) \n",
    "score_df.to_csv(\"resultats_smote.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "333d2c9757a75282c9d4867cccb346dd97fbdd6438562bec117fa31e687fe23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
