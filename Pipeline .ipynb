{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a638260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "import dask.dataframe as dd\n",
    "import fastparquet\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "try:\n",
    "    os.chdir(\"C:/Users/Sam/Documents/SISE/Fouille de données\")\n",
    "except:\n",
    "    os.chdir(\"/Users/titouanhoude/Documents/GitHub\")\n",
    "    \n",
    "train = pd.read_parquet('train.parquet.gzip')\n",
    "test = pd.read_parquet('test.parquet.gzip')\n",
    "\n",
    "Xtrain = train.drop([\"FlagImpaye\", \"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\", \"CodeDecision\", \"Unnamed: 0\"], axis = 1)\n",
    "\n",
    "Ytrain = pd.DataFrame(train.FlagImpaye)\n",
    "Ytrain = train['FlagImpaye'].astype('int')\n",
    "\n",
    "Xtest  = test.drop([\"FlagImpaye\",\"ZIBZIN\", \"Date\", \"Heure_split\", \"DateTransaction\",\"CodeDecision\", 'Unnamed: 0'], axis = 1)\n",
    "Ytest  = test.FlagImpaye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c2083f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# clf = ExtraTreesClassifier(n_estimators=50)\n",
    "# clf = clf.fit(Xtrain, Ytrain)\n",
    "# clf.feature_importances_\n",
    "\n",
    "# model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "# Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "# Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "# Xtrain = model.transform(train_100k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59539fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "\n",
    "# nbFoldValid = 5 # nombre de groupes pour la K-CV\n",
    "\n",
    "# models2={#'SVC': SVC(),\n",
    "#        'RandomForest': RandomForestClassifier(random_state=0),\n",
    "#        'XGBRFClassifier' : XGBRFClassifier(random_state=0),\n",
    "#        'GradientBoosting' : GradientBoostingClassifier(random_state=0),\n",
    "#         'GradientBoosting' : GradientBoostingClassifier(random_state=0)\n",
    "#        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e00dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application des algorithmes \n",
    "\n",
    "def applyAlgo(algo, Xtrain, Ytrain, Xtest, Ytest):\n",
    "\n",
    "    # On commence par indiquer ce que l'on va faire avec chaque algorithme.\n",
    "    # on prendra soin de préciser les hyper-paramètres dont dépend l'algorithme\n",
    "\n",
    "    if algo == \"SVC\":\n",
    "        clf = SVC()\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTrain = clf.predict(Xtrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"RandomForest\":\n",
    "        clf = RandomForestClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"GradientBoostingClassifier\":\n",
    "        clf = GradientBoostingClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"AdaBoostClassifier\":\n",
    "        clf = AdaBoostClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "        \n",
    "    elif algo == \"XGBRFClassifier\":\n",
    "        clf = XGBRFClassifier(random_state=0)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "    elif algo == \"KNeighborsClassifier\":\n",
    "        clf = KNeighborsClassifier(n_neighbors=1)\n",
    "        clf.fit(Xtrain, Ytrain)\n",
    "        rankTest = clf.predict(Xtest)\n",
    "\n",
    "   # Cette deuxième partie permet d'indiquer quelle est la mesure de performance que \n",
    "   # vous souhaitez considérer pour votre étude en cours\n",
    "    ctest = confusion_matrix(Ytest, rankTest)\n",
    "    ftest = round(2*ctest[1,1]/(2*ctest[1,1]+ctest[0,1]+ctest[1,0]),4)\n",
    "\n",
    "    return (ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c487a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Echantillon(number, Xtrain, Ytrain):\n",
    "    Xtrain = Xtrain.sample(n = number)\n",
    "    Ytrain = Ytrain.sample(n = number)\n",
    "\n",
    "    return number, Xtrain, Ytrain\n",
    "\n",
    "def Processing(method, Xtrain, Ytrain, Xtest):\n",
    "\n",
    "    if method == \"NoProcessing\":\n",
    "        pass\n",
    "    \n",
    "    if method == \"SelectFromModel\": \n",
    "        clf = ExtraTreesClassifier(n_estimators=50)\n",
    "        clf = clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "        model = SelectFromModel(clf, prefit=True)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[model.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[model.get_support(indices = True)]]\n",
    "\n",
    "    if method == \"Variance\": \n",
    "        selector = VarianceThreshold(threshold = 0.8)\n",
    "        selector.fit_transform(Xtrain)\n",
    "\n",
    "        Xtrain = Xtrain[Xtrain.columns[selector.get_support(indices = True)]]\n",
    "        Xtest = Xtest[Xtest.columns[selector.get_support(indices = True)]]\n",
    "\n",
    "    return method, Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec303b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_S(sampling, Xtrain, Ytrain):\n",
    "\n",
    "    sm = BorderlineSMOTE(sampling_strategy = sampling,random_state = 0)\n",
    "\n",
    "    Xtrain, Ytrain = sm.fit_resample(Xtrain, Ytrain)\n",
    "    \n",
    "    return sampling, Xtrain, Ytrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ececb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "# pipe_Sampling = Pipeline([('Smote', BorderlineSMOTE()), ('Modele',model)])\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "# from functions import loadCsv, oneHotEncodeColumns, data_recovery\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Normalisation\n",
    "# normalizer = Normalizer()\n",
    "# normalizer.fit(Xtrain)\n",
    "# Xtrain = normalizer.transform(Xtrain)\n",
    "# Xtest = normalizer.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64fab815",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m method : \n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models : \n\u001b[0;32m---> 21\u001b[0m         s, Xtrain, Ytrain \u001b[39m=\u001b[39m SMOTE_S(s, Xtrain, Ytrain)\n\u001b[1;32m     22\u001b[0m         smote\u001b[39m.\u001b[39mappend(s)\n\u001b[1;32m     24\u001b[0m         x, X_sample_train, Y_sample_train \u001b[39m=\u001b[39m Echantillon(x, Xtrain, Ytrain)\n",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m, in \u001b[0;36mSMOTE_S\u001b[0;34m(sampling, Xtrain, Ytrain)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mSMOTE_S\u001b[39m(sampling, Xtrain, Ytrain):\n\u001b[1;32m      3\u001b[0m     sm \u001b[39m=\u001b[39m BorderlineSMOTE(sampling_strategy \u001b[39m=\u001b[39m sampling,random_state \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     Xtrain, Ytrain \u001b[39m=\u001b[39m sm\u001b[39m.\u001b[39;49mfit_resample(Xtrain, Ytrain)\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m sampling, Xtrain, Ytrain\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/base.py:203\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m--> 203\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_resample(X, y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/base.py:84\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     81\u001b[0m arrays_transformer \u001b[39m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m     82\u001b[0m X, y, binarize_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling_strategy_ \u001b[39m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msampling_strategy, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sampling_type\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     88\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_resample(X, y)\n\u001b[1;32m     90\u001b[0m y_ \u001b[39m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m     label_binarize(output[\u001b[39m1\u001b[39m], classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39munique(y)) \u001b[39mif\u001b[39;00m binarize_y \u001b[39melse\u001b[39;00m output[\u001b[39m1\u001b[39m]\n\u001b[1;32m     92\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/utils/_validation.py:548\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[0;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[39mif\u001b[39;00m sampling_strategy \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m sampling_strategy \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    542\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhen \u001b[39m\u001b[39m'\u001b[39m\u001b[39msampling_strategy\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is a float, it should be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39min the range (0, 1]. Got \u001b[39m\u001b[39m{\u001b[39;00msampling_strategy\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         )\n\u001b[1;32m    546\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict(\n\u001b[1;32m    547\u001b[0m         \u001b[39msorted\u001b[39m(\n\u001b[0;32m--> 548\u001b[0m             _sampling_strategy_float(sampling_strategy, y, sampling_type)\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    549\u001b[0m         )\n\u001b[1;32m    550\u001b[0m     )\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m callable(sampling_strategy):\n\u001b[1;32m    552\u001b[0m     sampling_strategy_ \u001b[39m=\u001b[39m sampling_strategy(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/imblearn/utils/_validation.py:387\u001b[0m, in \u001b[0;36m_sampling_strategy_float\u001b[0;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[1;32m    381\u001b[0m     sampling_strategy_ \u001b[39m=\u001b[39m {\n\u001b[1;32m    382\u001b[0m         key: \u001b[39mint\u001b[39m(n_sample_majority \u001b[39m*\u001b[39m sampling_strategy \u001b[39m-\u001b[39m value)\n\u001b[1;32m    383\u001b[0m         \u001b[39mfor\u001b[39;00m (key, value) \u001b[39min\u001b[39;00m target_stats\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    384\u001b[0m         \u001b[39mif\u001b[39;00m key \u001b[39m!=\u001b[39m class_majority\n\u001b[1;32m    385\u001b[0m     }\n\u001b[1;32m    386\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m([n_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m n_samples \u001b[39min\u001b[39;00m sampling_strategy_\u001b[39m.\u001b[39mvalues()]):\n\u001b[0;32m--> 387\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe specified ratio required to remove samples \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfrom the minority class while trying to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgenerate new samples. Please increase the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mratio.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         )\n\u001b[1;32m    393\u001b[0m \u001b[39melif\u001b[39;00m sampling_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39munder-sampling\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    394\u001b[0m     n_sample_minority \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(target_stats\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[0;31mValueError\u001b[0m: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio."
     ]
    }
   ],
   "source": [
    "models = ['KNeighborsClassifier', 'GradientBoostingClassifier', 'RandomForest' ]# 'SVC']\n",
    "number = [100000, 380000 , 999999, 3899362]\n",
    "method = ['NoProcessing', 'SelectFromModel' , 'Variance']\n",
    "sampling = [0.3 , 0.5]\n",
    "\n",
    "import time\n",
    "Fmesure = []\n",
    "execution = []\n",
    "algo = []\n",
    "n_data = []\n",
    "variable_selector = []\n",
    "smote = []\n",
    "\n",
    "columns = [\"Classifieur\", \"Echantillon\", \"Selection Variable\", \"Performance\", \"Sampling\", \"Temps_execution\"] # rajouter pipeline ici si on veut comparer les pré processing\n",
    "\n",
    "for s in sampling :\n",
    "    for x in number :\n",
    "        for l in method : \n",
    "            for model in models : \n",
    "\n",
    "                s, X_sample_train, Y_sample_train = SMOTE_S(s, Xtrain, Ytrain)\n",
    "                smote.append(s)\n",
    "\n",
    "                x, X_sample_train, Y_sample_train = Echantillon(x, X_sample_train, Y_sample_train)\n",
    "                n_data.append(x)\n",
    "                \n",
    "                l, X_sample_train, X_sample_test = Processing(l, X_sample_train, Y_sample_train, Xtest)\n",
    "                variable_selector.append(l)\n",
    "\n",
    "                # Stocker l'algorithme qui tourne\n",
    "                algo.append(model)\n",
    "                start = time.time()\n",
    "                # Fin normalisation\n",
    "                apTest = applyAlgo(model, X_sample_train, Y_sample_train, X_sample_test, Ytest)\n",
    "                Fmesure.append(apTest) \n",
    "                end = time.time()\n",
    "                elapsed = end - start \n",
    "                execution.append(elapsed)\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(list(zip(algo, n_data, variable_selector, Fmesure, smote, execution)), columns= columns) \n",
    "score_df.to_csv(\"resultats_smote.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "333d2c9757a75282c9d4867cccb346dd97fbdd6438562bec117fa31e687fe23c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
